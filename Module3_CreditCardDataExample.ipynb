{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/ML_656/blob/main/Module3_CreditCardDataExample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Credit Card Default Case Study\n",
        "\n",
        "Dani Bauer, 2022"
      ],
      "metadata": {
        "id": "bFNuMreLe4xh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQkFLmYWqmN3"
      },
      "source": [
        "In this tutorial, we will introduce one of the examples we will use in the coming classes: We will attempt to predict defaults on credits cards using a public dataset.\n",
        "\n",
        "We will use logistic regression to predict default scores.\n",
        "\n",
        "## Credit Card Default Application\n",
        "\n",
        "We rely on the dataset `pa_data_UCI_Credit_Card.csv` from the UCI Machine Learning Repository (Lichman, M., 2013. [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science).  This datasets provides credit card defaults for customers in Taiwan.  We are given some demographic information ($X_1$-$X_5$), the previous history of payments ($X_6$-$X_{11}$), the amount of previous bills ($X_{12}$-$X_{17}$), and amounts of previous payments ($X_{18}$-$X_{23}$).  Finally, variable 24 is our target, whetyher there was a default in the next months.\n",
        "\n",
        "\n",
        "As always, let's start with importing the libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANNbSksYqWS3"
      },
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt  \n",
        "import pandas as pd \n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score, roc_curve, auc\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import scale"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the dataset"
      ],
      "metadata": {
        "id": "okvP8ddqfGwX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70QK-AnxqWTB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0bac13b-2cc3-4079-c775-57b5732e8fd3"
      },
      "source": [
        "!git clone https://github.com/danielbauer1979/ML_656.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ML_656'...\n",
            "remote: Enumerating objects: 141, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 141 (delta 12), reused 0 (delta 0), pack-reused 117\u001b[K\n",
            "Receiving objects: 100% (141/141), 23.32 MiB | 8.83 MiB/s, done.\n",
            "Resolving deltas: 100% (62/62), done.\n",
            "Checking out files: 100% (28/28), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzW2COB-qWTC"
      },
      "source": [
        "mydata = pd.read_csv('ML_656/UCI_Credit_Card.csv', index_col=0)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Exploration and Preparation"
      ],
      "metadata": {
        "id": "Y4q8flL3fLw8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ7xrrgfqWTC"
      },
      "source": [
        "mydata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at some aggregate statistics."
      ],
      "metadata": {
        "id": "bdiFv-p4fQ1j"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWYoH_ngqWTD"
      },
      "source": [
        "mydata.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, a number of the variables are included numerically but really they have factor character, particularly Gender (1 = male; 2 = female), Education (1 = graduate school; 2 = university; 3 = high school; 4 = others), Marital status (1 = married; 2 = single; 3 = others), and default payment. Let's store them as factors.  We will do the same for history of past payment ($X_6$-$X_{11}$), although they really have ordinal character."
      ],
      "metadata": {
        "id": "651lfbJMgE04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "factor = ['SEX', 'EDUCATION', 'MARRIAGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'default.payment.next.month']"
      ],
      "metadata": {
        "id": "ZAwjAtXxgJ9B"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, a number of the levels occur very sparsely: there are 11 levels for all the `PAY` variables, but only the first six seem to be frequent.  So let's collapse levels 7 through 11 to one:"
      ],
      "metadata": {
        "id": "23vcLxDYgRcb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr5L46eWqWTD"
      },
      "source": [
        "mydata['PAY_0'][mydata['PAY_0']>4] = 4\n",
        "mydata['PAY_2'][mydata['PAY_2']>4] = 4\n",
        "mydata['PAY_3'][mydata['PAY_3']>4] = 4\n",
        "mydata['PAY_4'][mydata['PAY_4']>4] = 4\n",
        "mydata['PAY_5'][mydata['PAY_5']>4] = 4\n",
        "mydata['PAY_6'][mydata['PAY_6']>4] = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we rescale the numerical columns, as we know a number of learners require scaled inputs (and it should not matter for others):"
      ],
      "metadata": {
        "id": "Q4xm1bOegjmw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rfdm9IdqWTE"
      },
      "source": [
        "mydata_numcols = mydata.drop(columns = factor)\n",
        "mydata_faccols = mydata[factor].drop(columns = ['default.payment.next.month']).astype('category')\n",
        "dummies = pd.get_dummies(mydata_faccols, drop_first=True)\n",
        "mydata_numcols_sc_0 = scale(mydata_numcols)\n",
        "mydata_numcols_sc = pd.DataFrame(data=mydata_numcols_sc_0, columns = mydata_numcols.columns, index = dummies.index)\n",
        "mydata_sc = pd.concat([mydata_numcols_sc, dummies], axis = 1)\n",
        "mydata_sc = pd.concat([mydata_sc, mydata['default.payment.next.month']], axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And Let's relabel the long name of the dependent variable:"
      ],
      "metadata": {
        "id": "tSOZn3i-gtKf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCaaHETUqWTF"
      },
      "source": [
        "mydata = mydata.rename(columns={\"default.payment.next.month\": \"default\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look:"
      ],
      "metadata": {
        "id": "fwfWHqovgynk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UfQEQiEqWTF"
      },
      "source": [
        "mydata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zibCpbmrqWTG"
      },
      "source": [
        "mydata.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check a correlation plot to make sure none of the variables is extremely correlated:"
      ],
      "metadata": {
        "id": "HdHdC-Kyg5A2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpsVMCpCqWTG"
      },
      "source": [
        "mask = np.triu(np.ones_like(mydata.corr(), dtype=bool))\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "sns.heatmap(mydata.corr(), mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it looks like the bill amounts are highly correlated.  Let's just keep the most recent one and then the average of all of them:\n"
      ],
      "metadata": {
        "id": "ihKWiDkphM_3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLIXJYlyqWTH"
      },
      "source": [
        "mydata.insert(17, \"BILL_AVG\", (mydata['BILL_AMT1']+mydata['BILL_AMT2']+mydata['BILL_AMT3']+mydata['BILL_AMT4']+mydata['BILL_AMT5']+mydata['BILL_AMT6'])/6, True) \n",
        "mydata = mydata.rename(columns={\"BILL_AMT1\": \"BILL_REC\"})\n",
        "del mydata['BILL_AMT2']\n",
        "del mydata['BILL_AMT3']\n",
        "del mydata['BILL_AMT4']\n",
        "del mydata['BILL_AMT5']\n",
        "del mydata['BILL_AMT6']\n",
        "mydata.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's save the dataset so that we can use it in coming tutorials without having to go through this procedure again:"
      ],
      "metadata": {
        "id": "IdJvKO_FhSkU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRIpqkhsqWTH"
      },
      "source": [
        "mydata.to_csv('pa_data_UCI_Credit_Card_prepped.csv') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predictive Modeling"
      ],
      "metadata": {
        "id": "Xwm41PjahXf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usually, let's split our dataset:"
      ],
      "metadata": {
        "id": "e80dFeKijLfn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK8HBxrBqWTI"
      },
      "source": [
        "Train, Test = train_test_split(mydata, test_size=0.25)\n",
        "Train_y = Train['default']\n",
        "Train = Train.drop(columns = ['default'])\n",
        "Test_y = Test['default']\n",
        "Test = Test.drop(columns = ['default'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run a logistic regression model:"
      ],
      "metadata": {
        "id": "l1BP7jjcjPzy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ5lP5pnBP7F"
      },
      "source": [
        "logistic_model1 = LogisticRegression(fit_intercept=True, max_iter=500).fit(Train,Train_y)\n",
        "print(logistic_model1.intercept_)\n",
        "print(logistic_model1.coef_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check predictions:"
      ],
      "metadata": {
        "id": "PxNquZnljYn2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egLqSR1RD1bb"
      },
      "source": [
        "logistic_pred_1 = logistic_model1.predict_proba(Test)\n",
        "np.sum(logistic_pred_1[:,1] > 0.5)\n",
        "np.sum(logistic_pred_1[:,1] > 0.38)\n",
        "logistic_pred_1_lab = logistic_pred_1[:,1] > 0.36\n",
        "confusion_matrix(Test_y, logistic_pred_1_lab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we are missing quite a few.  Let's condider the AUC:"
      ],
      "metadata": {
        "id": "4pC5K5zNjbXe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIY4iAiwqWTL"
      },
      "source": [
        "fpr, tpr, threshold = roc_curve(Test_y, logistic_pred_1[:,1])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}