{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/ML_656/blob/main/Module3_IntroductionToClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Classification\n",
        "Dani Bauer, 2022\n",
        "\n",
        "In this tutorial, we introduce two ways to think about probabilistically modeling class data: (i) either as modeling the probability of belonging to a certain class $j$ conditionally on observing the features; or (ii) as modeling the probability of observing a certain combination of features conditional to belonging to class $j$.  "
      ],
      "metadata": {
        "id": "Fc-m__Y8s4Y3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQkFLmYWqmN3"
      },
      "source": [
        "As always, let's start with importing the libraries:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANNbSksYqWS3"
      },
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt  \n",
        "import pandas as pd \n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score, roc_curve, auc\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import scale"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Two ways to think about class data\n",
        "\n",
        "There are two ways of thinking about class data: (i) the probability of belonging to a certain class is a function of explanatoy variables; and (ii) the explanatory variables are drawn from different distributions depending on the class.  Note that the first approach is the basis for generalized linear/additive models (GLM/GAM) for classification whereas the second way underlies the Naive Bayes classifier.\n",
        "\n",
        "### Review of Concepts I\n",
        "\n",
        "Assume we are given predictors $X_i$ and outcome variables $Y_i \\in \\{1,2,...,M\\}$.  Remember that we are trying to assess \n",
        "$$\n",
        "p_j(x_0)= \\mathbb{P}(Y=j|X=x_0).\n",
        "$$\n",
        "A sensible prediction is then the $j$ for given data $x_0$ such that $p_j(x_0)$ is the largest.  That's exactly the **Bayes classifier**.\n",
        "\n",
        "For instance, let's assume that $X$ is two-dimensional consisting of height and weight, and that we attempt to predict sex $Y$ -- say 1 for male and 0 for females.  Hence, $p(x)=p_1(x)$ is the probability for a male given height and weight, and the probability of being female is $p_0(x)=1-p(x).$\n",
        "\n",
        "One way is to guess a functional form for $p_j(x)$.  This is the idea behind **logistic regression**.  For instance, in our male female example we would have:\n",
        "$$\n",
        "\\text{Proability Male} = p(\\underbrace{\\text{height},\\text{weight}}_x)=\\frac{e^{\\beta_0+\\beta_1\\times\\text{height}+\\beta_2\\times\\text{weight}}}{1+e^{\\beta_0+\\beta_1\\times\\text{height}+\\beta_2\\times\\text{weight}}}.\n",
        "$$\n",
        "\n",
        "Generalized Additive Models (GAMs) works similarly but allows for more general (non-linear) functions in the exponential. \n",
        "\n",
        "### Simulating a Class Data Set via Logistic Regression Approach\n"
      ],
      "metadata": {
        "id": "sJMjNPSGtoGD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Let us simulate the covariates by sampling from a multivariate Normal distribution with expected height 66 inches and expected weight 175 lbs, and corresponding standard deviation 7 inches and 25 lbs, respectively.  We assume the correlation is 50%. "
      ],
      "metadata": {
        "id": "-Nf0UBpyt4s7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed = 1\n",
        "sampsize = 100\n",
        "mean = [66,175]\n",
        "cov = [[7**2,0.5*7*25], [0.5*7*25, 25**2]]\n",
        "X = np.random.multivariate_normal(mean, cov, sampsize)"
      ],
      "metadata": {
        "id": "qb0aQbxJt-u2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Let us then assume that both height and weight are positively associated with being male.  More precisely, let's assume that the probability follows a logistic regression model with parameters $\\beta_1$ and $\\beta_2$ (which are just chosen arbitrarily).  The dependent variable ($Y$) then states whether the individual is male or not. "
      ],
      "metadata": {
        "id": "zxnOknwgt8ai"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_vLVUwYqWS5"
      },
      "source": [
        "beta_1 = 0.15\n",
        "beta_2 = 0.03\n",
        "probX = np.exp(beta_1 * (X[:,0] - 66) + beta_2 * (X[:,1] - 175))/(1+np.exp(beta_1 * (X[:,0] - 66) + beta_2 * (X[:,1] - 175)))\n",
        "ISMALE = (probX > np.random.uniform(0,1,sampsize))\n",
        "GenderDF1 = pd.DataFrame({'ISMALE':ISMALE,'HEIGHT': X[:,0], 'WEIGHT': X[:,1]})"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Let us take a look at the data as a function of height and weight."
      ],
      "metadata": {
        "id": "NmM7GnjguKb9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1PByrBisBsv"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "# plot x,y data with c as the color vector, set the line width of the markers to 0\n",
        "ax.scatter(X[:,0], X[:,1], c= ISMALE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Review of Concepts II\n",
        "\n",
        "An alternative approach is to rely on Bayes theorem to express $p_j(x)$ in terms of the probability of observing $x$ given class $j$:\n",
        "$$\n",
        "p_j(x_0)=\\frac{\\pi_j\\times f_j(x_0)}{\\sum_k \\pi_k\\times f_k(x_0)},\n",
        "$$\n",
        "where $\\pi_k$ are the (unconditional) proportions for the different classes.\n",
        "\n",
        "For instance, in our gender example, it is proximate to assume $\\pi_0=\\pi_1=50\\%,$ so that:\n",
        "$$\n",
        "p(x) = \\frac{f_1(x)}{f_0(x)+f_1(x)},\n",
        "$$\n",
        "where $f_1(x)$ is the probability (density) for a given combination $x$ of height and weight given that the individual is a male and $f_0(x)$ is the probability for a given combination $x$ of height and weight given that the individual is a female.\n",
        "\n",
        "In **Linear Discriminant Analysis (LDA)**, the assumption is that $f_j(x)$ is Normal with mean $\\mu_j$ (and with a covariance matrix independent of the class), and we can determine the class-specific mean $\\mu_j$ by calculating the sample average.  Given data $x_0$, we then choose the class $j_0$ that maximizes $p_j(x_0)$ among all classes $j.$  Naive Bayes works similar, but we allow for more general distributions -- with the caveat that the different feature components $x_{k}$ are assumed to be independent.\n",
        "\n",
        "### Simulating a Class Data Set via drawing the (conditional) covariates\n",
        "\n",
        "1. Let us consider simulating a dataset of males and females.  Here let's assume that we have 50 males and females (for a total sample size of 100), where we draw the height and weights for  males and females separately.  More precisely, we assume that males have expected height and weight of 72 inches and 190 lbs, respectively, whereas females have an expected height of 60 inches and an expected weight of 160 lbs.  We assume that the standard deviations are the same as in the previous example. \n"
      ],
      "metadata": {
        "id": "BycWA39ruSVR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTL_LjxIqWS7"
      },
      "source": [
        "halfsampsize = 50\n",
        "mean_M = [72,190]\n",
        "mean_F = [60,160]\n",
        "cov = [[7**2,0.5*7*25], [0.5*7*25, 25**2]]\n",
        "X_M = np.random.multivariate_normal(mean_M, cov, halfsampsize)\n",
        "X_F = np.random.multivariate_normal(mean_F, cov, halfsampsize)\n",
        "X = np.concatenate((X_M, X_F), axis=0)\n",
        "ISMALE = np.concatenate((np.ones(halfsampsize,dtype=bool), np.zeros(halfsampsize,dtype=bool)))\n",
        "GenderDF2 = pd.DataFrame({'ISMALE':ISMALE,'HEIGHT': X[:,0], 'WEIGHT': X[:,1]})"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Let us take a look at the data as a function of height and weight."
      ],
      "metadata": {
        "id": "Jz35k4r9uc2a"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzc_fhvHM9Dl"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "# plot x,y data with c as the color vector, set the line width of the markers to 0\n",
        "ax.scatter(X[:,0], X[:,1], c= ISMALE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyVEnTxmM8pk"
      },
      "source": [
        "So the two data sets look similar.  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Think before executing**:\n",
        "\n",
        "* If we were to run logistic regression and LDA on the two datasets, which method do you expect to perform better on the two datasets?  \n",
        "\n",
        "* Try it, i.e. compare logistic regression and LDA based on the simulated data sets.  "
      ],
      "metadata": {
        "id": "RNkuE6dhuozw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egW2HwYuqWS9"
      },
      "source": [
        "Train, Test = train_test_split(GenderDF1, test_size=0.5, random_state=1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9trdvwXdtC5D"
      },
      "source": [
        "logistic_model1 = LogisticRegression(fit_intercept=True).fit(Train[['HEIGHT','WEIGHT']],Train['ISMALE'])\n",
        "print(logistic_model1.intercept_)\n",
        "print(logistic_model1.coef_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB_OfiSrK_fW"
      },
      "source": [
        "logistic_pred_1 = logistic_model1.predict(Test[['HEIGHT','WEIGHT']])\n",
        "confusion_matrix(Test['ISMALE'],logistic_pred_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anpFXqYrMU6x"
      },
      "source": [
        "precision_score(Test['ISMALE'],logistic_pred_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlqstGa1qWS_"
      },
      "source": [
        "lda_model_1 = LinearDiscriminantAnalysis()\n",
        "lda_model_1.fit(Train[['HEIGHT','WEIGHT']],Train['ISMALE'])\n",
        "lda_pred_1 = lda_model_1.predict(Test[['HEIGHT','WEIGHT']])\n",
        "confusion_matrix(Test['ISMALE'],lda_pred_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnVUjtzLqWTA"
      },
      "source": [
        "Train, Test = train_test_split(GenderDF2, test_size=0.5, random_state=1)\n",
        "logistic_model2 = LogisticRegression(fit_intercept=True).fit(Train[['HEIGHT','WEIGHT']],Train['ISMALE'])\n",
        "logistic_pred_2 = logistic_model1.predict(Test[['HEIGHT','WEIGHT']])\n",
        "confusion_matrix(Test['ISMALE'],logistic_pred_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmpzgxlxqWTA"
      },
      "source": [
        "lda_model_2 = LinearDiscriminantAnalysis()\n",
        "lda_model_2.fit(Train[['HEIGHT','WEIGHT']],Train['ISMALE'])\n",
        "lda_pred_2 = lda_model_2.predict(Test[['HEIGHT','WEIGHT']])\n",
        "confusion_matrix(Test['ISMALE'],lda_pred_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generally, we expect the model to fit better that better describes the underlying data generating process, so logistic regression for the first and LDA for the second data set. Whether that comes out depends on randomness! Check by modifying the random seed!\n"
      ],
      "metadata": {
        "id": "FMqjNsO8vCqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Application"
      ],
      "metadata": {
        "id": "S9hq-FQmvde0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the real-world performance in a dataset from a [study by Caroline Davis](https://www.sciencedirect.com/science/article/pii/019566639090096Q) that has data on the height and weight of 200 individuals."
      ],
      "metadata": {
        "id": "O6HjArdXvU7c"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r49dAbuLNki0"
      },
      "source": [
        "!git clone https://github.com/danielbauer1979/ML_656.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT2dwPKCNvzF"
      },
      "source": [
        "hw_data = pd.read_csv('ML_656/Davis.csv')\n",
        "hw_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l073F2eMOCBC"
      },
      "source": [
        "hw_data['sex'] = pd.get_dummies(hw_data['sex'],drop_first=True)\n",
        "hw_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F3aCUWYJqYf"
      },
      "source": [
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "# plot x,y data with c as the color vector, set the line width of the markers to 0\n",
        "ax.scatter(hw_data['height'], hw_data['weight'], c= hw_data['sex'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsMYtSm5Ok80"
      },
      "source": [
        "Train, Test = train_test_split(hw_data, test_size=0.5, random_state=1)\n",
        "logistic_model3 = LogisticRegression(fit_intercept=True).fit(Train[['height','weight']],Train['sex'])\n",
        "logistic_pred_3 = logistic_model3.predict(Test[['height','weight']])\n",
        "confusion_matrix(Test['sex'],logistic_pred_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1JYJhM6HvtG"
      },
      "source": [
        "lda_model_3 = LinearDiscriminantAnalysis()\n",
        "lda_model_3.fit(Train[['height','weight']],Train['sex'])\n",
        "lda_pred_3 = lda_model_3.predict(Test[['height','weight']])\n",
        "confusion_matrix(Test['sex'],lda_pred_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the performance is quite similar, which is no surprise given the similarity."
      ],
      "metadata": {
        "id": "4w9z4LLlvlnq"
      }
    }
  ]
}