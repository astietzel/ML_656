{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/ML_656/blob/main/Module10_Unsupervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Learning: Clustering and PCA Analysis\n",
        "Dani Bauer, 2022\n",
        "\n",
        "As mentioned in the beginning of lecture, there are (at least) two basic learning setups.  In **supervised** machine learning one observes a response $Y$ with observing $p$ different features $X=(X_1,X_2,\\ldots,X_p)$, where we typically postulate the relationship $Y = f(X)+\\varepsilon$ and $\\varepsilon$ independent of $X$ with mean zero. Here quality is usually assessed by the (test/out-of-sample) error that compares predictions and realizations for a separate dataset.  In **unsupervised** learning, we only observe $p$ features $X_1,X_2,\\ldots,X_p$, and we would like to learn about their relationship -- without focussing on a supervising outcome.  Of course, the difficulty is how to assess quality in this case -- so different unsupervised learning techniques are very different, and which one to pick will depend on the nature of the problem.\n",
        "\n",
        "In this tutorial, we will take a closer look at two algorithms: **Principal Component Analysis (PCA)** and **Clustering**.  There are variety of other techniques, including anomaly detection, self-organizing maps, association analysis, etc. We start with quick introductions but then look into the applications of these techniques in more detail in the context of a case study.\n",
        "\n",
        "As usual, we start by implementing the relevant packages:"
      ],
      "metadata": {
        "id": "aIiejbOjljzy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vCC-SQb7VRR"
      },
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt  \n",
        "import matplotlib.lines as mlines\n",
        "import plotly.figure_factory as ff\n",
        "import pandas as pd   \n",
        "import seaborn as sns\n",
        "from random import sample\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler # For rescaling metrics to fit 0 to 1 range\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.spatial.distance import euclidean"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principal Component Analysis\n",
        "\n",
        "## Background\n",
        "\n",
        "The key idea behind *Principal Component Analysis* (PCA) is to use \"meaningful\" linear combinations of the data:\n",
        "$$\n",
        "Z_m = \\sum_{j=1}^p \\phi_{jm} X_j \\text{ such that }\\sum_{j=1}^p \\phi^2_{jm} = 1,\n",
        "$$\n",
        "where the idea is to choose:\n",
        "\n",
        "- $\\phi_{j1},\\,j=1,\\ldots,p$  such that the the variance of $Z_1$ is maximal (i.e., so that it captures most of the variation in the $X$s)\n",
        "\n",
        "- $\\phi_{j2},\\,j=1,\\ldots,p$ such that the variance of $Z_2$ is maximial out of all the variables that are uncorrelated with $Z_1$.\n",
        "\n",
        "- $\\phi_{j3},\\,j=1,\\ldots,p$ such that the variance of $Z_3$ is maximial out of all the variables that are uncorrelated with $Z_1$ and $Z_2$. $\\ldots$\n",
        "\n",
        "Hence, one way to intepret the principal components are the linear combination that best reflect the variation in the data.\n",
        "\n",
        "Importantly, the scale of variables matters, so it is a good idea to center and scale your variables.  Also, the components are only determined up to their sign (plus/minus).  To discern how important different variables are, one typically considers the *Percent of Variance Explained* (PVE):\n",
        "$$\n",
        "\\text{PVE}_m = \\frac{\\sum_{i=1}^n \\left(\\sum_{j=1}^p \\phi_{jm}x_{ij}\\right)^2}{\\sum_{i=1}^n \\sum_{j=1}^p x_{ij}^2},\n",
        "$$\n",
        "and the resulting plot that depicts PVE by principal components is referred to as a *scee plot*.\n",
        "\n",
        "## Simulated Example\n",
        "\n",
        "Let's consider a basic example, where we simulate heights and weights of a fictional population according to some arbitrary parameters.  More precisely, we assume that weight and height follow Normal distributions with a mean weight (in kg) of 70 and a mean height (in cm) of 170m, and variances of 25 and 150, respectively.  We assume the correlation parameter is 50%.\n"
      ],
      "metadata": {
        "id": "M0g7gj9NmeKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mu = (70,170)\n",
        "cov = np.array([[25, np.sqrt(25)*np.sqrt(150)*0.5], [np.sqrt(25)*np.sqrt(150)*0.5, 150]])\n",
        "X_raw = np.random.multivariate_normal(mu, cov, size=5000)"
      ],
      "metadata": {
        "id": "WUPuLhokm73m"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check quick:"
      ],
      "metadata": {
        "id": "nwTiQwq_p0Fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_raw.mean(axis=0)"
      ],
      "metadata": {
        "id": "bl3Q6vB_prhP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3765e1ac-153e-4fec-b44f-491ffb71618b"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 70.09771694, 170.26494615])"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.cov(np.transpose(X_raw))"
      ],
      "metadata": {
        "id": "NmFiKEcHqCjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.corrcoef(np.transpose(X_raw))"
      ],
      "metadata": {
        "id": "7CP5fiTOyaJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the *empirical* moments look similar to our theoretical moments. Let's take a peak:\n"
      ],
      "metadata": {
        "id": "8qFiIVu4p161"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (6,4))\n",
        "plt.scatter(X_raw[:,0], X_raw[:,1], s = 1, color = 'black')\n",
        "plt.xlabel('weight')\n",
        "plt.ylabel('height')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tY1m2LNRpBg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prinicipal components are closely related to the eigen-analysis (eigenvalues and eigenvectors) of the correlation (or covariance) matrix.  Let's illustrate (if you never had a linear algebra class, feel free to skip this part).  First, let's scale the data and calculate the (empirical) correlation matrix -- remember, in practical settings we usually don't know the underlying parameters:\n"
      ],
      "metadata": {
        "id": "_IXLO-52q0zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "R = np.corrcoef(np.transpose(X_raw))\n",
        "R"
      ],
      "metadata": {
        "id": "5VahnsYuq1xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's calculate the eigenvalues and the eigenvectors of `R`.  As a reminder, the eigenvectors decompose a matrix (a.k.a. a linear mapping in finite dimensions) in orthogonal directions, whereas the eignvalues provide the \"length\" (importance) of the directions.  In particular, we can represent a symmetric matrix in diagnolized form by relying on eigenvectors and eigenvalues. "
      ],
      "metadata": {
        "id": "mdNuryMYrJt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dec = np.linalg.eig(R)\n",
        "Dec"
      ],
      "metadata": {
        "id": "rpMC6B0drS6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V = Dec[1] # Matrix of Eigenvectores\n",
        "D = np.diag(Dec[0]) # Diagonal Matrix of Eigenvalues\n",
        "np.dot(np.dot(V,D),np.transpose(V)) # Calculates V * D * V'"
      ],
      "metadata": {
        "id": "oGS9R-CxsVgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get intuition, let's plot the Eigenvectors:"
      ],
      "metadata": {
        "id": "yUlyHgUy3IyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_raw)\n",
        "plt.figure(figsize = (6,4))\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(scaler.transform(X_raw)[:,0], scaler.transform(X_raw)[:,1], s = 1, color = 'black')\n",
        "line1 = mlines.Line2D([0, 0.7071], [0, -.7071], color='red')\n",
        "line2 = mlines.Line2D([0, .7071], [0, .7071], color='yellow')\n",
        "transform = ax.transAxes\n",
        "ax.add_line(line1)\n",
        "ax.add_line(line2)\n",
        "plt.xlabel('scaled weight')\n",
        "plt.ylabel('scaled height')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_qNILkH44nBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is exactly this notion of \"importance\" that motivates principal components.  Indeed, the loadings of the principal components just amount to the *ordered* eigenvalues..."
      ],
      "metadata": {
        "id": "D100cSoIsTRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering\n",
        "\n",
        "## Background\n",
        "\n",
        "*Clustering* refers to techniques for finding subgroups in a given dataset. The typical approach to determine clusters $C_1,\\ldots,C_K$ is to minimize:\n",
        "$$\n",
        "\\sum_{k=1}^K W(C_k),\n",
        "$$\n",
        "where $W$ is a measure of *variation* within a cluster.  For instance, **k-means clustering** uses the Euclidean distance to measure variation:\n",
        "$$\n",
        "W(C_k) = \\frac{1}{|C_k|} \\sum_{i,i' \\in C_k} \\sum_{j=1}^p (x_{ij} - x_{i'j})^2.\n",
        "$$\n",
        "The algorithms are implemented via a greedy algorithm by considering the centers of clusters (referred to as *centroids*).  The number of clusters $K$ must be chosen beforehand.  One approach is *hierarchical clustering*, where one starts with a larger number of clusters and then *fuses* custers that are similar (e.g., with regards to the distance between their centroids). \n",
        "\n",
        "## Simulated Example\n",
        "\n",
        "Let's consider a very basic simulated example -- let's simulate normal random variables with different means:"
      ],
      "metadata": {
        "id": "OEru_Pl4ACGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_raw2 = np.random.multivariate_normal((0,0), np.array([[1, 0], [0, 1]]), size=100)\n",
        "X_raw2[0:49,0]=X_raw2[1:50,0]+3\n",
        "X_raw2[0:49,1]=X_raw2[1:50,1]-4"
      ],
      "metadata": {
        "id": "tCFUcnLqbdpU"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot:"
      ],
      "metadata": {
        "id": "Ozl7WSIvbdMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (6,4))\n",
        "plt.scatter(X_raw2[0:49,0], X_raw2[0:49,1], color='red')\n",
        "plt.scatter(X_raw2[50:99,0], X_raw2[50:99,1], color='black')\n",
        "plt.xlabel('X0')\n",
        "plt.ylabel('X1')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AG2cFW5VctpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's run k means clustering:\n"
      ],
      "metadata": {
        "id": "5WdTAU69d6rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters = 2, init = 'k-means++', max_iter = 1000, random_state = 123)\n",
        "kmeans.fit(X_raw2)\n",
        "centroids = kmeans.cluster_centers_\n",
        "centroids"
      ],
      "metadata": {
        "id": "oq_YzcFwd6dY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label = kmeans.fit_predict(X_raw2)\n",
        "label"
      ],
      "metadata": {
        "id": "PhY9123eecf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the algorithm was able to identify how we set up the data!"
      ],
      "metadata": {
        "id": "TPcmD4aOeq3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case Study: County Health Rankings 2013\n",
        "\n",
        "We analyze [County Health Rankings](www.countyhealthrankings.org) in the US in 2013, based on a data set from the University of Wisconsin Population Health Institute.  I took this exammple from Jim Guszcza, and he put this together in R (it's much better than what I had before :-).\n",
        "\n",
        "## Data Preparation\n",
        "\n",
        "Let's load the data:"
      ],
      "metadata": {
        "id": "3RW20yqheke7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge_n_HRNgRm0"
      },
      "source": [
        "!git clone https://github.com/danielbauer1979/ML_656.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2whMtWdEgtAh"
      },
      "source": [
        "health = pd.read_csv('ML_656/countyHealthRR.csv')"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health.info()"
      ],
      "metadata": {
        "id": "V3w077i9fShr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it's a large dataset, and the first three numbers are indices.  However, the remaning columns provide rather detailed information for each county.  Let's visualize: "
      ],
      "metadata": {
        "id": "W6DWnaeb0RHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(health.corr());"
      ],
      "metadata": {
        "id": "z8Ia-qwL0k1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So quite a bit of correlation, but nothing seems to be \"replicated.\"  \n",
        "\n",
        "Unfortunately, we have a bunch of missing data. Let's drop the columns where we have lots of missing values and then drop na-s:"
      ],
      "metadata": {
        "id": "4bfsz1CRe7y9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "012bX41Lg9dF"
      },
      "source": [
        "health = health.drop(columns=['FIPS','State','County','Perc.Fair.Poor.Health','Perc.Smokers','Perc.Excessive.Drinking','MV.Mortality.Rate','Pr.Care.Physician.Ratio','Perc.No.Soc.Emo.Support'])"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm0Hb3CxhytE"
      },
      "source": [
        "health = health.dropna()"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the resulting data:"
      ],
      "metadata": {
        "id": "AOvjhvUr1Me8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "health.info()"
      ],
      "metadata": {
        "id": "YHiF5HDW1PnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health.describe()"
      ],
      "metadata": {
        "id": "oeIESh-O1SwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's scale the data so as to make sure we are comparing apples to apples:"
      ],
      "metadata": {
        "id": "T2-2bu_51cH1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVSWGGQ-kVhg"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(health)\n",
        "health_sc = scaler.transform(health)\n",
        "health_sc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering\n",
        "\n",
        "### K-Means clustering\n",
        "\n",
        "Let's commence with k-means clustering, which is avalable in the library `sklearn.cluster` (along with other clustering algorithms, see below). To decide on the number of clusters, we evaluate how the within-sum-of-squares varies between the number of clusters using a so-called *elbow plot*:\n"
      ],
      "metadata": {
        "id": "VGmhd4rb1dlV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF4Tvu-imXH7"
      },
      "source": [
        "wcss = []\n",
        "for i in range(2, 12):\n",
        "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=1000, n_init=10, random_state=0)\n",
        "    kmeans.fit(health_sc)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "plt.bar(range(2, 12), wcss)\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('WCSS')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that six clusters seem to present a reasonable choice, there is a bit of an \"elbow\" there. So let's go with six:"
      ],
      "metadata": {
        "id": "LlIwaq1N3J4j"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J1JGhuVnqGv"
      },
      "source": [
        "kmeans = KMeans(n_clusters=6, init='k-means++', max_iter=1000, n_init=10, random_state=0)\n",
        "kmeans.fit(health_sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can get the labels via:"
      ],
      "metadata": {
        "id": "cOXho1k23ok6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans.labels_"
      ],
      "metadata": {
        "id": "jtsoEKP_3tLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add to our dataset:"
      ],
      "metadata": {
        "id": "fgE-k3VH9nLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "health['k_means_cl'] = kmeans.labels_"
      ],
      "metadata": {
        "id": "GJ_sfTvh9pE2"
      },
      "execution_count": 170,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the different clusters by evaluating the YPLL.Rate (Years of Potential Life Lost). "
      ],
      "metadata": {
        "id": "hHZYmnlK_Jrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_means_cl'] == 0]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "bG0zgwt5-XMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_means_cl'] == 1]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "HTplXWaf_eLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_means_cl'] == 2]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "ddf2dbEl_eON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_means_cl'] == 3]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "QZPQh5Xc_eRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_means_cl'] == 4]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "kq_DhtSp_eTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_means_cl'] == 5]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "pNqE83nb_n-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's relabel so it's increasing:"
      ],
      "metadata": {
        "id": "euotX4dR_tFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relab(label):\n",
        "  if label == 0:\n",
        "    return 0\n",
        "  elif label == 1:\n",
        "    return 1\n",
        "  elif label == 2:\n",
        "    return 5\n",
        "  elif label == 3:\n",
        "    return 2\n",
        "  elif label == 4:\n",
        "    return 4\n",
        "  else:\n",
        "    return 3"
      ],
      "metadata": {
        "id": "Umgy4pQ-F7m7"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmlab = list(map(relab, kmeans.labels_))"
      ],
      "metadata": {
        "id": "reVaZbJTHH08"
      },
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health['k_means_cl'] = kmlab"
      ],
      "metadata": {
        "id": "LeY28w_K5hGM"
      },
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hierarchical Clustering\n",
        "\n",
        "Let's use the hierarchical clustering algorithm in scikit (`AgglomerativeClustering`). Plotting a *dendrogram* is easier with the `scipy.cluster.hierarchy`, yet given the size of the dataset we don't see a ton here. \n",
        "\n",
        "To compare with the k-means approach, let's run a hierarchical clustering also with six clusters."
      ],
      "metadata": {
        "id": "yWrU6Bnl3u0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hierarch = AgglomerativeClustering(n_clusters=6)\n",
        "hierarch.fit(health_sc)"
      ],
      "metadata": {
        "id": "1oBxQTqs7p4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we can get the labels via:"
      ],
      "metadata": {
        "id": "gYDv_mNx8tTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hierarch.labels_"
      ],
      "metadata": {
        "id": "Ogxz292p8J9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also add to the dataset and carry out a similar approach as before:"
      ],
      "metadata": {
        "id": "Q3_hVGRN55AV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "health['k_hier_cl'] = hierarch.labels_"
      ],
      "metadata": {
        "id": "3mgy8-NK5-Ld"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_hier_cl'] == 0]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "ib6Xuvyo8p4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_hier_cl'] == 1]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "X9t8epBf8p1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_hier_cl'] == 2]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "lpS1mFd18pyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_hier_cl'] == 3]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "bMakZhkB8pq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_hier_cl'] == 4]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "d0sBEYLn8pgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_hier_cl'] == 5]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "Inm4wef88zpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relab2(label):\n",
        "  if label == 0:\n",
        "    return 5\n",
        "  elif label == 1:\n",
        "    return 2\n",
        "  elif label == 2:\n",
        "    return 0\n",
        "  elif label == 3:\n",
        "    return 4\n",
        "  elif label == 4:\n",
        "    return 3\n",
        "  else:\n",
        "    return 1"
      ],
      "metadata": {
        "id": "5cHreh5l9BK6"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hierlab = list(map(relab2, hierarch.labels_))"
      ],
      "metadata": {
        "id": "6rUa25gq9X_1"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health['k_hier_cl'] = hierlab "
      ],
      "metadata": {
        "id": "kgom-7Rl9hCS"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the two:"
      ],
      "metadata": {
        "id": "VzFnwZJW5yj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [0,1,2,3,4,5]\n",
        "confusion_matrix(health['k_means_cl'],health['k_hier_cl'],labels=labels)"
      ],
      "metadata": {
        "id": "u84XOdUS9qs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it looks like there is quite a bit of overlap, particularly with regards to clusters 0, 1, and 2. It appears clusters 4 and 5 are overlapping. However, it also is clear that assigning points to clusters isn't highly obvious."
      ],
      "metadata": {
        "id": "ps_F7hxy8JrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing Clusters\n",
        "\n",
        "Let's visualize clusters by comparing the distributions of some of the features across clusters. As we had discussed, comparing distributions can be conveniently accomplished via box-whisker plots.\n",
        "\n",
        "Let's start with children in poverty."
      ],
      "metadata": {
        "id": "X7GF4qiTA0Ll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x = \"k_means_cl\", y = \"Perc.Children.in.Poverty\", data = health)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HYKSMgk6_wJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that we sorted the clusters according to YPLL.Rate (Years of Potential Life Lost), so more years of life lost is a sign of worse health. It appears that this weakly aligns with children poverty.\n",
        "\n",
        "Let's look at violent crimes."
      ],
      "metadata": {
        "id": "0CDlt4LMBRi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x = \"k_means_cl\", y = \"Violent.Crime.Rate\", data = health)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BD-oXwLXBwtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So here we have a weak alignment, although cluster 4 seems a bit lower. Let's look at fast food concentration."
      ],
      "metadata": {
        "id": "gypA3GKUB9JR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x = \"k_means_cl\", y = \"Perc.Fast.Foods\", data = health)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yRJkARhMCQDt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}