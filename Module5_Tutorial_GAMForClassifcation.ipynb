{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/ML_656/blob/main/Module5_Tutorial_GAMForClassifcation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Credit Card Default Case Study\n",
        "\n",
        "Dani Bauer, 2022"
      ],
      "metadata": {
        "id": "bFNuMreLe4xh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQkFLmYWqmN3"
      },
      "source": [
        "In this tutorial, we will introduce one of the examples we will use in the coming tutorials: We will attempt to predict defaults on credits cards using a public dataset.\n",
        "\n",
        "We will apply several of our predictive models from the past weeks (glm and gam) but also check the performance of the *naive Bayes classifier*. \n",
        "\n",
        "## Review of Concepts and Maths\n",
        "\n",
        "As we saw in a previously, non-linear regression models are powerful approaches for depicting non-linear relationships -- with the key caveat that our explanatory variable had a single dimension only.  Once we venture into higher dimensions -- that means multiple, potentially interrelated features -- obtaining a similar fit will require a *very* large number of data points becaus of the **curse of dimensionality**.  Hence, statistical learning methods (need to) impose structure, in one way or another, and picking a learner in some way depends on picking an appropiate structure. **Generalized Additive Models** (GAMs) -- leverages the performance of non-linear regression models in lower dimensions but imposes an *additive* structure between the functions of the individual features:\n",
        "$$\n",
        "g\\left(\\mathbb{E}\\left[Y | X_1,...,X_p\\right]\\right) = \\alpha + f_1(X_1) + \\ldots + f_p(X_p).\n",
        "$$\n",
        "As such, GAMs take on an intermediate position between linear regression and a general non-linear model:  They generalize the impact each feature can have on the outcome, but they keep the same structure on their relationship.  In particular, they do not allow for rich interactions between the variables -- which is the key downside of GAMs.\n",
        "\n",
        "In the context of the binary classification problem discussed above, we can choose a logistic *link function* $g$ to predict the probability of success $p(X_1,...,X_p)$:\n",
        "$$\n",
        "\\log\\left\\{\\frac{p(X_1,...,X_p)}{1-p(X_1,...,X_p)}\\right\\} = \\alpha + f(X_1,...,X_p). \n",
        "$$\n",
        "but because of the **curse** this will require many many points.  A GAM assumes:\n",
        "$$\n",
        "\\log\\left\\{\\frac{p(X_1,...,X_p)}{1-p(X_1,...,X_p)}\\right\\} = \\alpha + f_1(X_1) + \\ldots + f_p(X_p). \n",
        "$$\n",
        "Hence, the GAM takes on an intermediate position between logistic regression and general non-linear classification models.\n",
        "\n",
        "As discussed, an alternative framing of binary classification is to model the feature probabilities conditional on class:\n",
        "$$\n",
        "f_j(X_1,...,X_p),\\,j=1,2.\n",
        "$$\n",
        "but because of the **curse** learning the *joint* distribution will require many many points.  The Naive Bayes Classifier assumes:\n",
        "$$\n",
        "f_j(X_1,...,X_p) = f_{j1}(X_1)\\times...\\times f_{jp}(X_p),\\,j=1,2,\n",
        "$$\n",
        "so that the distribuition of the features is conditionally independent.  This allows to focus on learning the *marginal* distribution, which -- much like for the GAM -- is much simpler because we can use all the data!  However, again this comes at a cost: the Naive Bayes classifier ignores interdependencies between features, which can be crucial in predictive modeling.\n",
        "\n",
        "To summarize, GAMs and the Naive Bayes Classifiers finesse the **curse** by imposing structure on the interaction and the dependence structure, respectively.  However, of course that will limit their performance if these assumptions are highly inappropriate.\n",
        "\n",
        "\n",
        "## Credit Card Default Application\n",
        "\n",
        "We rely on the dataset `pa_data_UCI_Credit_Card.csv` from the UCI Machine Learning Repository (Lichman, M., 2013. [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science).  This datasets provides credit card defaults for customers in Taiwan.  We are given some demographic information ($X_1$-$X_5$), the previous history of payments ($X_6$-$X_{11}$), the amount of previous bills ($X_{12}$-$X_{17}$), and amounts of previous payments ($X_{18}$-$X_{23}$).  Finally, variable 24 is our target, whetyher there was a default in the next months.\n",
        "\n",
        "\n",
        "As always, let's start with importing the libraries:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pygam"
      ],
      "metadata": {
        "id": "8aOk8EV7dgd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANNbSksYqWS3"
      },
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt  \n",
        "import pandas as pd \n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score, roc_curve, auc\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import scale\n",
        "from pygam import LogisticGAM, LinearGAM, GAM, s, f, l\n",
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the dataset"
      ],
      "metadata": {
        "id": "okvP8ddqfGwX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70QK-AnxqWTB"
      },
      "source": [
        "!git clone https://github.com/danielbauer1979/ML_656.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzW2COB-qWTC"
      },
      "source": [
        "mydata = pd.read_csv('ML_656/UCI_Credit_Card.csv', index_col=0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Exploration and Preparation"
      ],
      "metadata": {
        "id": "Y4q8flL3fLw8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ7xrrgfqWTC"
      },
      "source": [
        "mydata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at some aggregate statistics."
      ],
      "metadata": {
        "id": "bdiFv-p4fQ1j"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWYoH_ngqWTD"
      },
      "source": [
        "mydata.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, a number of the variables are included numerically but really they have factor character, particularly Gender (1 = male; 2 = female), Education (1 = graduate school; 2 = university; 3 = high school; 4 = others), Marital status (1 = married; 2 = single; 3 = others), and default payment. Let's store them as factors.  We will do the same for history of past payment ($X_6$-$X_{11}$), although they really have ordinal character."
      ],
      "metadata": {
        "id": "651lfbJMgE04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "factor = ['SEX', 'EDUCATION', 'MARRIAGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 'default.payment.next.month']"
      ],
      "metadata": {
        "id": "ZAwjAtXxgJ9B"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, a number of the levels occur very sparsely: there are 11 levels for all the `PAY` variables, but only the first six seem to be frequent.  So let's collapse levels 7 through 11 to one:"
      ],
      "metadata": {
        "id": "23vcLxDYgRcb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr5L46eWqWTD"
      },
      "source": [
        "mydata['PAY_0'][mydata['PAY_0']>4] = 4\n",
        "mydata['PAY_2'][mydata['PAY_2']>4] = 4\n",
        "mydata['PAY_3'][mydata['PAY_3']>4] = 4\n",
        "mydata['PAY_4'][mydata['PAY_4']>4] = 4\n",
        "mydata['PAY_5'][mydata['PAY_5']>4] = 4\n",
        "mydata['PAY_6'][mydata['PAY_6']>4] = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we rescale the numerical columns, as we know a number of learners require scaled inputs (and it should not matter for others):"
      ],
      "metadata": {
        "id": "Q4xm1bOegjmw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rfdm9IdqWTE"
      },
      "source": [
        "mydata_numcols = mydata.drop(columns = factor)\n",
        "mydata_faccols = mydata[factor].drop(columns = ['default.payment.next.month']).astype('category')\n",
        "dummies = pd.get_dummies(mydata_faccols, drop_first=True)\n",
        "mydata_numcols_sc_0 = scale(mydata_numcols)\n",
        "mydata_numcols_sc = pd.DataFrame(data=mydata_numcols_sc_0, columns = mydata_numcols.columns, index = dummies.index)\n",
        "mydata_sc = pd.concat([mydata_numcols_sc, dummies], axis = 1)\n",
        "mydata_sc = pd.concat([mydata_sc, mydata['default.payment.next.month']], axis = 1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And Let's relabel the long name of the dependent variable:"
      ],
      "metadata": {
        "id": "tSOZn3i-gtKf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCaaHETUqWTF"
      },
      "source": [
        "mydata = mydata.rename(columns={\"default.payment.next.month\": \"default\"})"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look:"
      ],
      "metadata": {
        "id": "fwfWHqovgynk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UfQEQiEqWTF"
      },
      "source": [
        "mydata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zibCpbmrqWTG"
      },
      "source": [
        "mydata.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check a correlation plot to make sure none of the variables is extremely correlated:"
      ],
      "metadata": {
        "id": "HdHdC-Kyg5A2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpsVMCpCqWTG"
      },
      "source": [
        "mask = np.triu(np.ones_like(mydata.corr(), dtype=bool))\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "sns.heatmap(mydata.corr(), mask=mask, cmap=cmap, vmax=.3, center=0,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it looks like the bill amounts are highly correlated.  Let's just keep the most recent one and then the average of all of them:\n"
      ],
      "metadata": {
        "id": "ihKWiDkphM_3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLIXJYlyqWTH"
      },
      "source": [
        "mydata.insert(17, \"BILL_AVG\", (mydata['BILL_AMT1']+mydata['BILL_AMT2']+mydata['BILL_AMT3']+mydata['BILL_AMT4']+mydata['BILL_AMT5']+mydata['BILL_AMT6'])/6, True) \n",
        "mydata = mydata.rename(columns={\"BILL_AMT1\": \"BILL_REC\"})\n",
        "del mydata['BILL_AMT2']\n",
        "del mydata['BILL_AMT3']\n",
        "del mydata['BILL_AMT4']\n",
        "del mydata['BILL_AMT5']\n",
        "del mydata['BILL_AMT6']\n",
        "mydata.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's save the dataset so that we can use it in coming tutorials without having to go through this procedure again:"
      ],
      "metadata": {
        "id": "IdJvKO_FhSkU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRIpqkhsqWTH"
      },
      "source": [
        "mydata.to_csv('pa_data_UCI_Credit_Card_prepped.csv') "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predictive Modeling"
      ],
      "metadata": {
        "id": "Xwm41PjahXf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usually, let's split our dataset:"
      ],
      "metadata": {
        "id": "e80dFeKijLfn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK8HBxrBqWTI"
      },
      "source": [
        "Train, Test = train_test_split(mydata, test_size=0.25)\n",
        "Train_y = Train['default']\n",
        "Train = Train.drop(columns = ['default'])\n",
        "Test_y = Test['default']\n",
        "Test = Test.drop(columns = ['default'])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run a logistic regression model:"
      ],
      "metadata": {
        "id": "l1BP7jjcjPzy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ5lP5pnBP7F"
      },
      "source": [
        "logistic_model1 = LogisticRegression(fit_intercept=True, max_iter=500).fit(Train,Train_y)\n",
        "print(logistic_model1.intercept_)\n",
        "print(logistic_model1.coef_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check predictions:"
      ],
      "metadata": {
        "id": "PxNquZnljYn2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egLqSR1RD1bb"
      },
      "source": [
        "logistic_pred_1 = logistic_model1.predict_proba(Test)\n",
        "np.sum(logistic_pred_1[:,1] > 0.5)\n",
        "np.sum(logistic_pred_1[:,1] > 0.38)\n",
        "logistic_pred_1_lab = logistic_pred_1[:,1] > 0.36\n",
        "confusion_matrix(Test_y, logistic_pred_1_lab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we are missing quite a few.  Let's condider the AUC:"
      ],
      "metadata": {
        "id": "4pC5K5zNjbXe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIY4iAiwqWTL"
      },
      "source": [
        "fpr, tpr, threshold = roc_curve(Test_y, logistic_pred_1[:,1])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see if we can build a GAM that beats the linear learners.  We use some of the information from the GLM, and ignore some of the insignificant variables.  We include non-linear effects in the significant continuous features, and also interact `PAY_0` with `LIMIT_BAL` -- simply because that seems to be a good idea.  Remember, incorporating intuition is legitimate and even important given the *No Free Lunch Theorem* (of course, we could improve on the GLMs as well by using interaction terms)."
      ],
      "metadata": {
        "id": "UX4ppWSQjieW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gam = LogisticGAM(s(0,n_splines=5) + l(1) + f(2) + f(3) + l(4) + f(5) + f(6) + f(7) + f(8) + f(9) + f(10) + s(11, n_splines=5) + s(12, n_splines=5) + s(13, n_splines=5) + l(14) + l(15) + l(16)).fit(Train, Train_y)\n",
        "XX = gam.generate_X_grid\n",
        "plt.rcParams['figure.figsize'] = (28, 8)\n",
        "fig, axs = plt.subplots(1, 17)\n",
        "titles = list(Train.columns)\n",
        "for i, ax in enumerate(axs):\n",
        "    XX = gam.generate_X_grid(term=i)\n",
        "    ax.plot(XX[:, i], gam.partial_dependence(term=i, X=XX))\n",
        "    ax.plot(XX[:, i], gam.partial_dependence(term=i, X=XX, width=.95)[1], c='r', ls='--')\n",
        "    ax.set_title(titles[i]);\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BuN17cp8SInB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check if it does better:"
      ],
      "metadata": {
        "id": "R7f3lNM6joLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gam_preds = gam.predict_proba(Test)\n",
        "gam_pred_labels = np.zeros(len(Test_y))\n",
        "gam_pred_labels[gam_preds >0.5] = 1\n",
        "confusion_matrix(Test_y, gam_pred_labels)"
      ],
      "metadata": {
        "id": "OdB8F3GiSMe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like it!\n",
        "\n",
        "Let's check the AUC:"
      ],
      "metadata": {
        "id": "mxQQDqXvjrX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fpr, tpr, threshold = roc_curve(Test_y, gam_preds)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c7i7SdD3SUMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's try a Naive Bayes Classifier:"
      ],
      "metadata": {
        "id": "soOptsWJkcg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_model1 = GaussianNB()\n",
        "nb_model1 = nb_model1.fit(Train,Train_y)\n",
        "nb_pred_1 = nb_model1.predict(Test)\n",
        "confusion_matrix(Test_y, nb_pred_1)"
      ],
      "metadata": {
        "id": "2X3WOvp6VW33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doesn't look promising...\n",
        "\n",
        "Let's check the AUC:"
      ],
      "metadata": {
        "id": "Mdm-lSuHWEVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_pred_1 = nb_model1.predict_proba(Test)\n",
        "fpr, tpr, threshold = roc_curve(Test_y, nb_pred_1[:,1])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HubvRpkIVZOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it doesn't perform too well here."
      ],
      "metadata": {
        "id": "6LmFAMX0WL6G"
      }
    }
  ]
}