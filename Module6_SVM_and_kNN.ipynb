{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/ML_656/blob/main/Module6_SVM_and_kNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Credit Card Default Case Study\n",
        "\n",
        "Dani Bauer, 2022"
      ],
      "metadata": {
        "id": "bFNuMreLe4xh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup -- let's load the necesary packages."
      ],
      "metadata": {
        "id": "Tu5PTQVn9bm2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANNbSksYqWS3"
      },
      "source": [
        "import numpy as np \n",
        "import matplotlib.pyplot as plt  \n",
        "import pandas as pd \n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression \n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score, roc_curve, auc\n",
        "from sklearn import preprocessing\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQkFLmYWqmN3"
      },
      "source": [
        "In this tutorial, we introduce and illustrate *k-nearest-neighbor classifier*  (kNN) and *support-vector-machine* (SVM) classfiers. We first introduce the conceps and then go back to predicting defaults on credits cards using a public dataset.\n",
        "\n",
        "## Review of Concepts and Maths -- SVM\n",
        "\n",
        "The intellectual starting point for SVMs is that:\n",
        "$$\n",
        "f(x) = \\beta_0 + \\beta_1\\,x_1 + \\beta_2\\,x_2 + \\ldots + \\beta_p\\,x_p = 0\n",
        "$$\n",
        "defines a *hyperplane*, and we can define a *binary linear classifier* according to \n",
        "$$\n",
        "f(x^{(0)}) \\begin{array}{c} >\\\\<\\end{array} 0,\n",
        "$$ \n",
        "(where points fall relative to that hyperplane).  We can determine a hyperplane that maximizes the distance to the two classes, allowing for exceptions.  It turns out that only a few points where the distances are sharp or that are exceptions matter, these are called *support vectors*.\n",
        "\n",
        "This idea generalizes to non-linear functions $f(\\cdot)$:\n",
        "$$\n",
        "f(x) = 0\n",
        "$$\n",
        "defines a boundary, and we can classify according to $$f(x^{(0)}) \\begin{array}{c} >\\\\<\\end{array} 0.$$  SVMs use *kernel functions* to define non-linear $f$'s that work particularly well."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVM Illustration\n",
        "\n",
        "Let's consider some arbitrary predictors in two-dimensional space, $x_1$ and $x_2$:"
      ],
      "metadata": {
        "id": "WSuIOh199WwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(1)\n",
        "x_1 = 2 * np.random.normal(0, 1, 100)\n",
        "x_2 = 2 * np.random.normal(0, 1, 100)"
      ],
      "metadata": {
        "id": "LuYl0R3x9WQM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we discussed above, the equation $f(x_1,x_2) = 0$ fixes a boundary in the two-dimensional space, so the \"true\" classification may be done according to:\n",
        "$$\n",
        "f(x_1,x_2) \\;\\; \\begin{array}{c}>\\\\< \\end{array} \\;\\; 0.\n",
        "$$\n",
        "We rely on the example that was used in generating the (non-linear) plot from lecture: "
      ],
      "metadata": {
        "id": "rEE2XopT-lJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = np.sign(5 * np.sin(0.5 * (x_1 - 2)) + 2 - x_2)\n",
        "mydata_true = pd.DataFrame({'y_true':y_true,'x_1':x_1,'x_2':x_2})"
      ],
      "metadata": {
        "id": "KsO-cGhE-Zma"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate more realistic data sets, where the classification is not sharp along the boundary, let's add noise.  We consider two different examples, one with less noise -- labeled 1 -- and one with more noise -- labeled 2."
      ],
      "metadata": {
        "id": "PBq5dqe3-ljm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_1 = np.sign(5 * np.sin(0.5 * (x_1 - 2)) + 2 - x_2 + 1 * np.random.normal(0,1, 100))\n",
        "mydata_1 = pd.DataFrame({'y_1':y_1,'x_1':x_1,'x_2':x_2})"
      ],
      "metadata": {
        "id": "yQgDC7fD-20e"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_2 = np.sign(5 * np.sin(0.5 * (x_1 - 2)) + 2 - x_2 + 3.5 * np.random.normal(0,1, 100))\n",
        "mydata_2 = pd.DataFrame({'y_2':y_2,'x_1':x_1,'x_2':x_2})"
      ],
      "metadata": {
        "id": "YScS003g_Qlo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also set up a function for plotting:"
      ],
      "metadata": {
        "id": "1iP1ju7k_ivy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_svc(svc, X, y, h=0.02, pad=0.25):\n",
        "  x_min, x_max = X[:, 0].min()-pad, X[:, 0].max()+pad\n",
        "  y_min, y_max = X[:, 1].min()-pad, X[:, 1].max()+pad\n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "  Z = svc.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "  Z = Z.reshape(xx.shape)\n",
        "  plt.contourf(xx, yy, Z, cmap=plt.cm.Paired, alpha=0.2)\n",
        "  plt.scatter(X[:,0], X[:,1], s=70, c=y, cmap=plt.cm.Paired)\n",
        "  # Support vectors indicated in plot by vertical lines\n",
        "  sv = svc.support_vectors_\n",
        "  plt.scatter(sv[:,0], sv[:,1], c='k', marker='|', s=100, linewidths='1')\n",
        "  plt.xlim(x_min, x_max)\n",
        "  plt.ylim(y_min, y_max)\n",
        "  plt.xlabel('X1')\n",
        "  plt.ylabel('X2')\n",
        "  plt.show()\n",
        "  print('Number of support vectors: ', svc.support_.size)"
      ],
      "metadata": {
        "id": "M8RRx2hi_Y-U"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start out with the first example, where there is little noise such that the classes are well-separated.  Let's run our first SVM classifier:"
      ],
      "metadata": {
        "id": "J1akhp2g_DXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = mydata_1.drop(columns = ['y_1']).values\n",
        "y = mydata_1['y_1'].values\n",
        "svc = SVC(kernel='rbf', gamma=1 ,C=10) # Equivalent to radial in R.\n",
        "svc.fit(X, y)\n",
        "plot_svc(svc, X, y)"
      ],
      "metadata": {
        "id": "ywmDk1xi_yp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "We can see that the SVM does well -- consider the confusion matrix:"
      ],
      "metadata": {
        "id": "leuNPM-XAR8K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = svc.predict(X)\n",
        "table = pd.DataFrame({'y_1':y_1,'pred':pred})\n",
        "table.groupby(['y_1','pred']).size().unstack('y_1').fillna(0)"
      ],
      "metadata": {
        "id": "VwYS8yUbAJ9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not a single misclassification in the training data.\n",
        "\n",
        "Even relative to the ground truth without noise the performance is good:"
      ],
      "metadata": {
        "id": "B7swdLreARRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "table = pd.DataFrame({'y_true':y_true,'pred':pred})\n",
        "table.groupby(['y_true','pred']).size().unstack('y_true')"
      ],
      "metadata": {
        "id": "Fik-Qdo1BFZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Very few misses!\n",
        "\n",
        "Now let's look at the second data set where the data is less well-separated.  Let's again fit the SVM, using the same choices (after all, the \"true\" pattern is the same):"
      ],
      "metadata": {
        "id": "P83ztMW3AeKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = mydata_2.drop(columns = ['y_2']).values\n",
        "y = mydata_2['y_2'].values\n",
        "svc = SVC(kernel='rbf', gamma=1 ,C=10) # Equivalent to radial in R.\n",
        "svc.fit(X, y)\n",
        "plot_svc(svc, X, y)"
      ],
      "metadata": {
        "id": "RGUegKciBKGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It does not look as good. It seems to overfit. Let's evaluate:"
      ],
      "metadata": {
        "id": "A1QrYP1cBb03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = svc.predict(X)\n",
        "table = pd.DataFrame({'y_2':y_2,'pred':pred})\n",
        "table.groupby(['y_2','pred']).size().unstack('y_2').fillna(0)"
      ],
      "metadata": {
        "id": "E7F5LxQ1BfYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "table = pd.DataFrame({'y_true':y_true,'pred':pred})\n",
        "table.groupby(['y_true','pred']).size().unstack('y_true')"
      ],
      "metadata": {
        "id": "G_EBxFWzBmcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hence, the error rate is substantially larger, and the number of support vectors is significantly greater.\n",
        "\n",
        "Overall, it appears that SVM is doing particularly well in the first context where the classes are well separated."
      ],
      "metadata": {
        "id": "OX_H3LwCBZvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Review of Concepts and Maths - KNN\n",
        "\n",
        "Another so-called *algorithmic* learners use different structural assumptions. For instance, we illustrate a **k-nearest neighbor (knn)** approach, where the predicted class at a point $x_0$ is chosen based on the $k$ points that are closest:\n",
        "$$\n",
        "y(x_0) = \\max_j\\left\\{\\frac{1}{K} \\sum_{i \\in N_K(x_0)} 1_{\\{y_i=j\\}}\\right\\},\n",
        "$$\n",
        "where $N_k(x_0)$ denotes the index set of the $K$ points in the training sample that are closest to the point $x_0$ (usually in the sense of Euclidean distance).  This is very differnt than what we have seen before in that we don't have an underlying \"probabilistic\" approach.\n",
        "\n",
        "## Credit Card Default Application\n",
        "\n",
        "We go back rely on the dataset `pa_data_UCI_Credit_Card.csv` from the UCI Machine Learning Repository (Lichman, M., 2013. [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science).  This datasets provides credit card defaults for customers in Taiwan.  We are given some demographic information ($X_1$-$X_5$), the previous history of payments ($X_6$-$X_{11}$), the amount of previous bills ($X_{12}$-$X_{17}$), and amounts of previous payments ($X_{18}$-$X_{23}$).  Finally, variable 24 is our target, whetyher there was a default in the next months.\n",
        "\n",
        "\n",
        "As always, let's start with importing the libraries:"
      ],
      "metadata": {
        "id": "zX3MAB598ru5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the dataset"
      ],
      "metadata": {
        "id": "okvP8ddqfGwX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70QK-AnxqWTB"
      },
      "source": [
        "!git clone https://github.com/danielbauer1979/ML_656.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzW2COB-qWTC"
      },
      "source": [
        "mydata = pd.read_csv('ML_656/UCI_Credit_Card_prepped.csv', index_col=0)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predictive Modeling"
      ],
      "metadata": {
        "id": "Xwm41PjahXf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usually, let's split our dataset:"
      ],
      "metadata": {
        "id": "e80dFeKijLfn"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK8HBxrBqWTI"
      },
      "source": [
        "Train, Test = train_test_split(mydata, test_size=0.25)\n",
        "Train_y = Train['default']\n",
        "Train = Train.drop(columns = ['default'])\n",
        "Test_y = Test['default']\n",
        "Test = Test.drop(columns = ['default'])"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run a logistic regression model as a comparison:"
      ],
      "metadata": {
        "id": "l1BP7jjcjPzy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ5lP5pnBP7F"
      },
      "source": [
        "logistic_model1 = LogisticRegression(fit_intercept=True, max_iter=500).fit(Train,Train_y)\n",
        "print(logistic_model1.intercept_)\n",
        "print(logistic_model1.coef_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check predictions:"
      ],
      "metadata": {
        "id": "PxNquZnljYn2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egLqSR1RD1bb"
      },
      "source": [
        "logistic_pred_1 = logistic_model1.predict_proba(Test)\n",
        "np.sum(logistic_pred_1[:,1] > 0.5)\n",
        "np.sum(logistic_pred_1[:,1] > 0.38)\n",
        "logistic_pred_1_lab = logistic_pred_1[:,1] > 0.36\n",
        "confusion_matrix(Test_y, logistic_pred_1_lab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we are missing quite a few.  Let's condider the AUC:"
      ],
      "metadata": {
        "id": "4pC5K5zNjbXe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIY4iAiwqWTL"
      },
      "source": [
        "fpr, tpr, threshold = roc_curve(Test_y, logistic_pred_1[:,1])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "plt.legend(loc = 'lower right')\n",
        "plt.plot([0, 1], [0, 1],'r--')\n",
        "plt.xlim([0, 1])\n",
        "plt.ylim([0, 1])\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's re-run the GAM:"
      ],
      "metadata": {
        "id": "UX4ppWSQjieW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the knn classifier:"
      ],
      "metadata": {
        "id": "yrAs83vDj0Hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(Train, Train_y)\n",
        "Test_y_knn = knn_model.predict(Test)\n",
        "confusion_matrix(Test_y, Test_y_knn)"
      ],
      "metadata": {
        "id": "RYAqm5Ikj8vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it doesn't look it works too well here."
      ],
      "metadata": {
        "id": "soOptsWJkcg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's fun a naive svm (note: this takes a bit):"
      ],
      "metadata": {
        "id": "WNzkIQLLDjgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "svm = SVC(C=1.0, kernel='rbf', gamma=1)\n",
        "svm.fit(Train, Train_y) "
      ],
      "metadata": {
        "id": "45LlpLS0DqU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's evaluate:"
      ],
      "metadata": {
        "id": "h2TIsT7RES6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = svm.predict(Test)\n",
        "table = pd.DataFrame({'y_test':Test_y,'pred':pred})\n",
        "table.groupby(['y_test','pred']).size().unstack('y_test')"
      ],
      "metadata": {
        "id": "q8hwCOkVD7D7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So not great, we have to tune. Using cross validation, we can try (this takes a looong time):"
      ],
      "metadata": {
        "id": "nry0LSV4EZtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_parameters = [{'C': [0.01, 0.1, 1, 10, 100],'gamma': [0.5, 1,2,3,4]}]\n",
        "clf = GridSearchCV(SVC(kernel='rbf'), tuned_parameters, cv=10,scoring='accuracy', return_train_score=True)\n",
        "clf.fit(Train, Train_y)"
      ],
      "metadata": {
        "id": "Mf5_eTRNE5hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.best_params_"
      ],
      "metadata": {
        "id": "oeCA5RElFDqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix(Test_y, clf.best_estimator_.predict(Test))"
      ],
      "metadata": {
        "id": "V6sNNr4PFJgm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}